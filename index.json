[{"authors":["Ruhul Amin","Alisa Yurovsky","Yingtao Tian","Steven Skiena"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"72d9d7aa7e9e151faa345b54468eab20","permalink":"https://ruhulsbu.github.io/publication/bcb2018/","publishdate":"2017-02-01T00:00:00Z","relpermalink":"/publication/bcb2018/","section":"publication","summary":"Genome annotation is the process of labeling DNA sequences of an organism with its biological features, and is one of the fundamental problems in Bioinformatics. In this paper, we evaluate DNA K-mer embeddings and the application of RNNs for genome annotation. We show how to improve the performance of our deep networks by incorporating intermediate objectives and downstream algorithms to achieve better accuracy.","tags":null,"title":"DeepAnnotator: Genome Annotation with Deep Learning","type":"publication"},{"authors":["Fatehmeh Almodaresi","Lyle Ungar","Vivek Kulkarni","Mohsen Zakeri","Salvatore Giorgi","H Andrew Schwartz"],"categories":null,"content":"","date":1485907200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1485907200,"objectID":"873606e6daf47e6b962741033086d9f4","permalink":"https://ruhulsbu.github.io/publication/acl2017/","publishdate":"2017-02-01T00:00:00Z","relpermalink":"/publication/acl2017/","section":"publication","summary":"Natural language processing has increasingly moved from modeling documents and words toward studying the people behind the language. This move to working with data at the user or community level has presented the field with different characteristics of linguistic data. In this paper, we empirically characterize various lexical distributions at different levels of analysis, showing that, while most features are decidedly sparse and non-normal at the message-level (as with traditional NLP), they follow the central limit theorem to become much more Log-normal or even Normal at the userand county-levels. Finally, we demonstrate that modeling lexical features for the correct level of analysis leads to marked improvements in common social scientific prediction tasks.","tags":null,"title":"On the Distribution of Lexical Features at Multiple Levels of Analysis","type":"publication"},{"authors":["Prateek Jain","Vivek Kulkarni","Abhradeep Guha Thakurta","Oliver Williams"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"fea612e7e4682d376287f865c5c41da3","permalink":"https://ruhulsbu.github.io/publication/dropout/","publishdate":"2015-01-01T00:00:00Z","relpermalink":"/publication/dropout/","section":"publication","summary":"Training deep belief networks (DBNs) requires optimizing a non-convex function with an extremely large number of parameters. Dropout is a popular heuristic that has been practically shown to avoid local minima when training these networks. We investigate the robustness and stability properties of Dropout. We empirically validate our stability assertions for dropout in the context of convex ERMs and show that surprisingly, dropout significantly outperforms (in terms of prediction accuracy) the L2 regularization based methods for several benchmark datasets.","tags":null,"title":"To drop or not to drop: Robustness, consistency and differential privacy properties of dropout","type":"publication"}]